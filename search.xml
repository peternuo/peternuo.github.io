<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>pytorch从0构建线性回归模型</title>
      <link href="/2022/09/16/pytorch%E4%BB%8E0%E6%9E%84%E5%BB%BA%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B/"/>
      <url>/2022/09/16/pytorch%E4%BB%8E0%E6%9E%84%E5%BB%BA%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B/</url>
      
        <content type="html"><![CDATA[<p><em>参考：《动手学深度学习》——李沐</em></p><h4 id="模型原理"><a href="#模型原理" class="headerlink" title="模型原理"></a>模型原理</h4><p>模型定义：(以二维变量为例)</p><script type="math/tex; mode=display">\hat{y}=w_1x_1+w_2x_2+b</script><p>​    构建的模型得到的$\hat{y}$是对真实数据$y$的估计，假设采集的样本数为$n$，索引为$i$的样本特征为$x_1^{(i)}$和$x_2^{(i)}$，标签为$y^{(i)}$则有：</p><script type="math/tex; mode=display">\hat{y}^{(i)}=w_1x_1^{(i)}+w_2x_2^{(i)}+b</script><p>定义损失函数：平方误差</p><script type="math/tex; mode=display">l(w_1,w_2,b)=\frac{1}{n}\sum_{i=1}^{n}{l^{(i)}(w_1,w_2,b)}=\frac{1}{n}\sum_{i=1}^{n}{(w_1x_1^{(i)}+w_2x_2^{(i)}+b-y^{(i)})^2}</script><p>​    在模型训练中，我们希望找出⼀组模型参数，记为$w_1^<em>$，$w_2^</em>$，$b^*$来使训练样本平均损失最小：</p><script type="math/tex; mode=display">w_1^*,w_2^*,b=\underset{w_1,w_2,b}{arg\min} l\left( w_1,w_2,b \right)</script><p>如上述公式可知，通常，<strong>我们用训练数据集中所有样本误差的平均来衡量模型预测的质量</strong></p><p>​    但对于大量的数据时，采用所有样本往往使得计算量变大。因此将采用小批量随机梯度下降法(BGD)。</p><h4 id="优化方法"><a href="#优化方法" class="headerlink" title="优化方法"></a>优化方法</h4><p>​    此处需要一定的凸优化基础，可以参考《convex Optimization》—Boyd and L. Vandenberghe</p><h5 id="梯度下降法-批量梯度下降法"><a href="#梯度下降法-批量梯度下降法" class="headerlink" title="梯度下降法(批量梯度下降法)"></a>梯度下降法(批量梯度下降法)</h5><p>​    我们考虑如下优化目标。</p><script type="math/tex; mode=display">w^*=\underset{w}{arg\min} f(w,x)</script><p>​    其中$x$为给定的数据，其中$f$可以为任意损失函数。</p><p>​    因此由梯度下降法可以得到，下一个迭代点为：</p><script type="math/tex; mode=display">w_{n+1}=w_n-\alpha\frac{\partial f(w,x)}{\partial w}</script><p>可以看出，<strong>此处用了所有的数据</strong>，由于$\hat{y}$可以改写为$\hat{y}=[x_1,x_2,1]<em>[w_1,w_2,b]^T=x</em>w^T$，其中$w=[w_1,w_2,b]$，所以我们将考虑优化函数：(此处$\frac{1}{2}$只是为了最终求解的梯度较为好看)</p><script type="math/tex; mode=display">w^*=\underset{w}{arg\min}\frac{\partial f(w,x)}{\partial w}=\underset{w}{arg\min}\frac{1}{n}\sum_{i=1}^{n}{\frac{1}{2}(x^{(i)}*w^T-y^{(i)})^2}</script><p>​    可以看出此问题是一个凸问题，存在全局最优解，采用梯度下降法是可以得到最优解的，求梯度可得：</p><script type="math/tex; mode=display">\frac{\partial f(w,x)}{\partial w}=\frac{\partial \frac{1}{n}\sum_{i=1}^{n}{\frac{1}{2}(x^{(i)}*w^T-y^{(i)})^2}}{\partial w}=\frac{1}{n}\sum_{i=1}^{n}x^{(i)}(x^{(i)}*w^T-y^{(i)})</script><p>​    可以看出，上述的下降梯度用上了所有的数据，此方法存在的缺点则是：<strong>计算量大</strong>。存在的优点则是：相比于SGD和BGD<strong>下降速度较快</strong>。</p><h5 id="随机梯度下降法-SGD"><a href="#随机梯度下降法-SGD" class="headerlink" title="随机梯度下降法(SGD)"></a>随机梯度下降法(SGD)</h5><p><strong>算法原理：</strong>（用随机一个梯度近似全体梯度）</p><p>​    随机梯度下降法则是随机选取一个样本，进行梯度计算，进行参数更新。</p><p>​    即将上述$\frac{\partial f(w,x)}{\partial w}$改为：只用一个样本进行计算梯度</p><script type="math/tex; mode=display">\frac{\partial f(w,x)}{\partial w}=x^{(i)}(x^{(i)}*w^T-y^{(i)})</script><p><strong>收敛性</strong>：(我还没理解透，凸优化和概率论之间的联系还没搞清楚，听说是依期望收敛)</p><p>​    <em>参考：<a href="https://zhuanlan.zhihu.com/p/277709879">浅谈随机梯度下降&amp;小批量梯度下降 - 知乎 (zhihu.com)</a></em></p><p>​    假设我们样本量为$n$，则优化函数可以写出：</p><script type="math/tex; mode=display">w^*=\underset{w}{arg\min} f(w,x)=\underset{w}{arg\min} \frac{1}{n}(f_1(w,x^{(1)})+f_2(w,x^{(2)})+\cdots+f_n(w,x^{(n)}))</script><p>​    其中$f$为损失函数，此处有点滥用符号，但此式子不难理解，即<strong>对所有的损失加起来的最终值进行求最小</strong>。</p><p>​    例如：$f$是平方误差损失即$f=\frac{1}{n}\sum_{i=1}^{n}{\frac{1}{2}(x^{(i)}<em>w-y^{(i)})^2}$，则$f_i={\frac{1}{2}(x^{(i)}</em>w-y^{(i)})^2}$。</p><p>​    则有：</p><script type="math/tex; mode=display">\frac{\partial f(w,x)}{\partial w}=\frac{1}{n}(\frac{\partial f_1(w,x^{(1)})}{\partial w}+\frac{\partial f_2(w,x^{(2)})}{\partial w}+\cdots+\frac{\partial f_n(w,x^{(n)})}{\partial w})</script><p>​    随机梯度下降法则随机选取一个：</p><script type="math/tex; mode=display">\frac{\partial f_i(w,x^{(i)})}{\partial w}</script><p>​    代替$\frac{\partial f(w,x)}{\partial w}$进行参数更新。</p><p><strong>优缺点：</strong></p><p>​    优点：计算量小。</p><p>​    缺点：参数更新慢。</p><h5 id="小批量随机梯度下降法-BGD"><a href="#小批量随机梯度下降法-BGD" class="headerlink" title="小批量随机梯度下降法(BGD)"></a>小批量随机梯度下降法(BGD)</h5><p><strong>算法原理：</strong>是随机梯度缺点的改进，优点的牺牲。利用小批量样本进行梯度更新。</p><p>​    小批量随机梯度下降法则是随机选取小批量的样本，进行梯度计算，进行参数更新。</p><p>​    即将上述$\frac{\partial f(w,x)}{\partial w}$改为：用小批量样本进行计算梯度。</p><script type="math/tex; mode=display">\frac{\partial f(w,x)}{\partial w}=\frac{1}{|\mathcal{B}|}\sum_{i\in \mathcal{B}}x^{(i)}(x^{(i)}*w-y^{(i)})</script><p><strong>收敛性</strong>：和随机梯度下降原理一样。</p><p><strong>优缺点</strong>：是梯度下降和随机梯度下降的中和。</p><h4 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h4><p>先上代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入库</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> IPython <span class="keyword">import</span> display</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="comment"># 创建数据</span></span><br><span class="line">num_inputs = <span class="number">2</span></span><br><span class="line">num_examples = <span class="number">1000</span></span><br><span class="line">true_w = [<span class="number">2</span>, -<span class="number">3.4</span>]<span class="comment"># 给定数据权重</span></span><br><span class="line">true_b = <span class="number">4.2</span><span class="comment"># 给定参数b</span></span><br><span class="line">features = torch.randn(num_examples,num_inputs</span><br><span class="line">                       ,dtype=torch.float32)<span class="comment"># 利用randn创建随机数据</span></span><br><span class="line">labels = true_w[<span class="number">0</span>] * features[:, <span class="number">0</span>] + true_w[<span class="number">1</span>] * features[:, <span class="number">1</span>]<span class="comment"># 获取回归数据</span></span><br><span class="line">labels += torch.tensor(np.random.normal(<span class="number">0</span>, <span class="number">0.01</span>, size=labels.size()),</span><br><span class="line">                       dtype=torch.float32)<span class="comment"># 加入扰动</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 对创建的数据进行可视化</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">use_svg_display</span>():</span><br><span class="line">    <span class="comment"># 用矢量图显示</span></span><br><span class="line">  display.set_matplotlib_formats(<span class="string">&#x27;svg&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">set_figsize</span>(<span class="params">figsize=(<span class="params"><span class="number">3.5</span>, <span class="number">2.5</span></span>)</span>):</span><br><span class="line">    use_svg_display()</span><br><span class="line">    <span class="comment"># 设置图的尺寸</span></span><br><span class="line">    plt.rcParams[<span class="string">&#x27;figure.figsize&#x27;</span>] = figsize</span><br><span class="line">set_figsize()</span><br><span class="line">plt.scatter(features[:, <span class="number">1</span>].numpy(), labels.numpy(), <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建数据迭代器，进行批量学习</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">data_iter</span>(<span class="params">batch_size, features, labels</span>):</span><br><span class="line">    <span class="comment"># batch_size是选取的每次学习的数据长度</span></span><br><span class="line">    <span class="comment"># features是数据特征</span></span><br><span class="line">    <span class="comment"># labels是数据标签</span></span><br><span class="line">    num_examples = <span class="built_in">len</span>(features)</span><br><span class="line">    indices = <span class="built_in">list</span>(<span class="built_in">range</span>(num_examples))<span class="comment"># 获取索引</span></span><br><span class="line">    random.shuffle(indices)<span class="comment"># 打乱索引</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, num_examples, batch_size):</span><br><span class="line">        <span class="comment"># 最后⼀次可能不⾜⼀个batch可利用min(i + batch_size,num_examples)进行拆解</span></span><br><span class="line">        <span class="comment"># 将indices[i:min(i + batch_size,num_examples)]转为LongTensor类型</span></span><br><span class="line">        j = torch.LongTensor(indices[i:<span class="built_in">min</span>(i + batch_size,num_examples)])</span><br><span class="line">        <span class="comment"># 利用yield关键字生成迭代器</span></span><br><span class="line">        <span class="keyword">yield</span> features.index_select(<span class="number">0</span>, j), labels.index_select(<span class="number">0</span>, j)</span><br><span class="line"><span class="comment"># 定义线性回归模型</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">linreg</span>(<span class="params">X,w,b</span>):</span><br><span class="line">    <span class="comment"># 利用torch.mm进行矩阵相乘</span></span><br><span class="line">    <span class="keyword">return</span> torch.mm(X,w) + b</span><br><span class="line"><span class="comment"># 定义平方误差</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">squared_loss</span>(<span class="params">y_hat, y</span>):</span><br><span class="line">    <span class="keyword">return</span> (y_hat - y.view(y_hat.size())) ** <span class="number">2</span> / <span class="number">2</span></span><br><span class="line"><span class="comment"># 随机小批量梯度下降法</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sgd</span>(<span class="params">params, lr, batch_size</span>):</span><br><span class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">        param.data -= lr * param.grad / batch_size</span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="comment"># 利用random初始化w</span></span><br><span class="line">    w = torch.tensor(np.random.normal(<span class="number">0</span>,<span class="number">0.01</span>,(num_inputs,<span class="number">1</span>)), dtype=torch.float32)</span><br><span class="line">    <span class="comment"># 初始化b</span></span><br><span class="line">    b = torch.zeros(<span class="number">1</span>, dtype=torch.float32)</span><br><span class="line">    <span class="comment"># 将w和b设置为可追踪操作，requires_grad=True</span></span><br><span class="line">    w.requires_grad_(requires_grad=<span class="literal">True</span>)</span><br><span class="line">    b.requires_grad_(requires_grad=<span class="literal">True</span>)</span><br><span class="line">    lr = <span class="number">0.03</span> <span class="comment"># 设置学习率</span></span><br><span class="line">    num_epochs = <span class="number">3</span> <span class="comment"># 设置学习轮数 </span></span><br><span class="line">    net = linreg <span class="comment"># 定义线性模型</span></span><br><span class="line">    loss = squared_loss <span class="comment"># 定义损失函数</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        <span class="keyword">for</span> X,y <span class="keyword">in</span> data_iter(batch_size, features, labels): <span class="comment"># 从迭代器中选取每次batch的训练数据</span></span><br><span class="line">            l = loss(net(X,w,b),y).<span class="built_in">sum</span>() <span class="comment"># 求损失函数值</span></span><br><span class="line">            l.backward() <span class="comment"># 利用backward进行求导，得到的梯度会保存再w,b的grad属性中</span></span><br><span class="line">            sgd([w,b],lr,batch_size) <span class="comment"># 利用小批量梯度下降法进行更新</span></span><br><span class="line">            w.grad.data.zero_() <span class="comment"># grad.data.zero_()将梯度清零，不然梯度会累加</span></span><br><span class="line">            b.grad.data.zero_() <span class="comment"># grad.data.zero_()将梯度清零，不然梯度会累加</span></span><br><span class="line">        train_1 = loss(net(features,w,b),labels)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;epoch %d, loss %f&#x27;</span> % (epoch + <span class="number">1</span>, train_1.mean().item()))</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>输出：</p><p><img src="C:\Users\97196\Desktop\9699cc708a98e2ebcdef01e5ad0e9d3.jpg" alt="9699cc708a98e2ebcdef01e5ad0e9d3"></p><h4 id="代码解读"><a href="#代码解读" class="headerlink" title="代码解读"></a>代码解读</h4><p>​    只对主体代码进行解读。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建数据迭代器，进行批量学习</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">data_iter</span>(<span class="params">batch_size, features, labels</span>):</span><br><span class="line">    <span class="comment"># batch_size是选取的每次学习的数据长度</span></span><br><span class="line">    <span class="comment"># features是数据特征</span></span><br><span class="line">    <span class="comment"># labels是数据标签</span></span><br><span class="line">    num_examples = <span class="built_in">len</span>(features)</span><br><span class="line">    indices = <span class="built_in">list</span>(<span class="built_in">range</span>(num_examples))<span class="comment"># 获取索引</span></span><br><span class="line">    random.shuffle(indices)<span class="comment"># 打乱索引</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, num_examples, batch_size):</span><br><span class="line">        <span class="comment"># 最后⼀次可能不⾜⼀个batch可利用min(i + batch_size,num_examples)进行拆解</span></span><br><span class="line">        <span class="comment"># 将indices[i:min(i + batch_size,num_examples)]转为LongTensor类型</span></span><br><span class="line">        j = torch.LongTensor(indices[i:<span class="built_in">min</span>(i + batch_size,num_examples)])</span><br><span class="line">        <span class="comment"># 利用yield关键字生成迭代器</span></span><br><span class="line">        <span class="keyword">yield</span> features.index_select(<span class="number">0</span>, j), labels.index_select(<span class="number">0</span>, j)</span><br></pre></td></tr></table></figure><p>​    <strong>data_iter</strong>是从原始数据中产生小批量$\mathcal{B}$数据迭代器的函数。</p><p>​    其中yield关键字是产生迭代器的关键字，具体可以查看<a href="https://juejin.cn/post/7142090372250861599">python基础查漏补缺 - 掘金 (juejin.cn)</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义线性回归模型</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">linreg</span>(<span class="params">X,w,b</span>):</span><br><span class="line">    <span class="comment"># 利用torch.mm进行矩阵相乘</span></span><br><span class="line">    <span class="keyword">return</span> torch.mm(X,w) + b</span><br><span class="line"><span class="comment"># 定义平方误差</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">squared_loss</span>(<span class="params">y_hat, y</span>):</span><br><span class="line">    <span class="keyword">return</span> (y_hat - y.view(y_hat.size())) ** <span class="number">2</span> / <span class="number">2</span></span><br></pre></td></tr></table></figure><p>​    <strong>linreg</strong>，<strong>squared_loss</strong>是进行前向传播的神经网络层</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">    <span class="keyword">for</span> X,y <span class="keyword">in</span> data_iter(batch_size, features, labels): <span class="comment"># 从迭代器中选取每次batch的训练数据</span></span><br><span class="line">        l = loss(net(X,w,b),y).<span class="built_in">sum</span>() <span class="comment"># 求损失函数值</span></span><br><span class="line">        l.backward() <span class="comment"># 利用backward进行求导，得到的梯度会保存再w,b的grad属性中</span></span><br><span class="line">        sgd([w,b],lr,batch_size) <span class="comment"># 利用小批量梯度下降法进行更新</span></span><br><span class="line">        w.grad.data.zero_() <span class="comment"># grad.data.zero_()将梯度清零，不然梯度会累加</span></span><br><span class="line">        b.grad.data.zero_() <span class="comment"># grad.data.zero_()将梯度清零，不然梯度会累加</span></span><br><span class="line">    train_1 = loss(net(features,w,b),labels)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;epoch %d, loss %f&#x27;</span> % (epoch + <span class="number">1</span>, train_1.mean().item()))</span><br></pre></td></tr></table></figure><p>​    上述流程是关键。</p><p>​    Step1：（正向传播）用loss(net(X,w,b),y).sum()求得最终损失值。</p><p>​    <strong>Step2：</strong>（反向传播）根据w，b定义了<strong>requires_grad=True</strong>可以进行梯度计算，并把计算储存在w，b中的grad属性。</p><p>​    Step3：利用小批量随机梯度下降法进行参数更新。</p><p>​    Step4：对w，b进行梯度清0，否则梯度会累加。</p><p>​    Step5：对选取的小批量数据完成Step1到Step4循环后，进行下一个epoch。</p><h4 id="线性回归的简洁实现"><a href="#线性回归的简洁实现" class="headerlink" title="线性回归的简洁实现"></a>线性回归的简洁实现</h4><blockquote><p>torch.utils.data 模块提供了有关数据处理的⼯具</p><p>torch.nn 模块定义了⼤量神经⽹络的层</p><p>torch.nn.init 模块定义了各种初始化⽅法</p><p>torch.optim 模块提供了模型参数初始化的各种⽅法</p></blockquote><p>先上代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> init<span class="comment"># 初始化模型参数</span></span><br><span class="line"><span class="keyword">import</span> torch.utils.data <span class="keyword">as</span> Data <span class="comment"># 导入生成迭代器的库</span></span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line">torch.manual_seed(<span class="number">1</span>) <span class="comment"># 设置随机数种子</span></span><br><span class="line">torch.set_default_tensor_type(<span class="string">&#x27;torch.FloatTensor&#x27;</span>)<span class="comment"># 修改默认tensor类型</span></span><br><span class="line"><span class="comment"># 生成数据</span></span><br><span class="line">num_inputs = <span class="number">2</span></span><br><span class="line">num_examples = <span class="number">1000</span></span><br><span class="line">true_w = [<span class="number">2</span>, -<span class="number">3.4</span>]</span><br><span class="line">true_b = <span class="number">4.2</span></span><br><span class="line">features = torch.tensor(np.random.normal(<span class="number">0</span>,<span class="number">1</span>,(num_examples,num_inputs)),dtype=torch.<span class="built_in">float</span>)</span><br><span class="line">labels = true_w[<span class="number">0</span>] * features[:, <span class="number">0</span>] + true_w[<span class="number">1</span>] * features[:, <span class="number">1</span>] + true_b</span><br><span class="line">labels += torch.tensor(np.random.normal(<span class="number">0</span>, <span class="number">0.01</span>, size=labels.size()), dtype=torch.<span class="built_in">float</span>)</span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">10</span> <span class="comment"># 设置</span></span><br><span class="line">dataset = Data.TensorDataset(features, labels)</span><br><span class="line"><span class="comment"># 生成迭代器，shuffle为是否打乱，num_workers多线程</span></span><br><span class="line">data_iter= Data.DataLoader(dataset=dataset,batch_size=batch_size,shuffle=<span class="literal">True</span>,num_workers=<span class="number">2</span>)</span><br><span class="line"><span class="comment"># 定义模型</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LinearNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, n_feature</span>):</span><br><span class="line">        <span class="built_in">super</span>(LinearNet , self).__init__()<span class="comment"># 继承LinearNet中的初始化方式</span></span><br><span class="line">        self.linear= nn.Linear(n_feature,<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self ,x</span>):<span class="comment"># 正向传播</span></span><br><span class="line">        y= self.linear(x)</span><br><span class="line">        <span class="keyword">return</span> y</span><br><span class="line">net = LinearNet(num_inputs)</span><br><span class="line"><span class="comment"># 初始化模型参数</span></span><br><span class="line">init.normal(net.linear.weight, mean=<span class="number">0.0</span>, std=<span class="number">0.01</span>)</span><br><span class="line">init.constant(net.linear.bias, val=<span class="number">0.0</span>)</span><br><span class="line"><span class="comment"># 定义损失函数</span></span><br><span class="line">loss = nn.MSELoss()</span><br><span class="line"><span class="comment"># 定义优化方式</span></span><br><span class="line">optimizer= optim.SGD(net.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line"><span class="comment"># 训练</span></span><br><span class="line">num_epochs = <span class="number">3</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>,num_epochs + <span class="number">1</span>):</span><br><span class="line">    <span class="keyword">for</span> X,y <span class="keyword">in</span> data_iter:</span><br><span class="line">        output = net(X)</span><br><span class="line">        l = loss(output , y.view(-<span class="number">1</span>,<span class="number">1</span>))<span class="comment"># 正向传播</span></span><br><span class="line">        optimizer.zero_grad()<span class="comment"># 梯度清0</span></span><br><span class="line">        l.backward()<span class="comment"># 计算梯度</span></span><br><span class="line">        optimizer.step()<span class="comment"># 更新参数</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;epoch %d, loss: %f&#x27;</span> %(epoch, l.item()))</span><br></pre></td></tr></table></figure><p>代码解读：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 初始化模型参数</span></span><br><span class="line">init.normal(net.linear.weight, mean=<span class="number">0.0</span>, std=<span class="number">0.01</span>)</span><br><span class="line">init.constant(net.linear.bias, val=<span class="number">0.0</span>)</span><br></pre></td></tr></table></figure><p>不进行初始化也可以，pytorch自带有参数初始化。</p><p><strong>优化器</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">optimizer= optim.SGD(net.parameters(), lr=<span class="number">0.01</span>)</span><br></pre></td></tr></table></figure><p>optimizer中保存了net中的参数，因此当l.backward()执行的时候，梯度会存储在grad中，即在optimizer中，因此可以用step进行更新，注意此处的lr是对全局全部参数更新的学习率</p><p>如果想对某层网络的学习率进行调整，可以以下方式。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">optimizer =optim.SGD([</span><br><span class="line"> <span class="comment"># 如果对某个参数不指定学习率，就使⽤最外层的默认学习率</span></span><br><span class="line"> &#123;<span class="string">&#x27;params&#x27;</span>: net.subnet1.parameters()&#125;, <span class="comment"># lr=0.03</span></span><br><span class="line"> &#123;<span class="string">&#x27;params&#x27;</span>: net.subnet2.parameters(), <span class="string">&#x27;lr&#x27;</span>: <span class="number">0.01</span>&#125;</span><br><span class="line"> ], lr=<span class="number">0.03</span>)</span><br></pre></td></tr></table></figure><p><strong>模型构建</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义模型</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LinearNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, n_feature</span>):</span><br><span class="line">        <span class="built_in">super</span>(LinearNet , self).__init__()<span class="comment"># 继承LinearNet中的初始化方式</span></span><br><span class="line">        self.linear= nn.Linear(n_feature,<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self ,x</span>):<span class="comment"># 正向传播</span></span><br><span class="line">        y= self.linear(x)</span><br><span class="line">        <span class="keyword">return</span> y</span><br></pre></td></tr></table></figure><p>此处模型的定义用了继承的方式，定义了类继承了线性模型。</p><p>其中forward会在net(X)执行的时候自动调用，因为nn.Module中定义了__call__，当net(X)执行时候会自动调用。</p><p>除了上述定义模型的方法还可以创建Sequential</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 写法一</span></span><br><span class="line">net = nn.Sequential(</span><br><span class="line">    nn.Linear(num_inputs, <span class="number">1</span>)</span><br><span class="line">    <span class="comment"># 此处还可以传入其他层</span></span><br><span class="line">    )</span><br><span class="line"><span class="comment"># 写法二</span></span><br><span class="line">net = nn.Sequential()</span><br><span class="line">net.add_module(<span class="string">&#x27;linear&#x27;</span>, nn.Linear(num_inputs, <span class="number">1</span>))</span><br><span class="line"><span class="comment"># net.add_module ......</span></span><br><span class="line"><span class="comment"># 写法三</span></span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> OrderedDict</span><br><span class="line">net = nn.Sequential(OrderedDict([</span><br><span class="line">          (<span class="string">&#x27;linear&#x27;</span>, nn.Linear(num_inputs, <span class="number">1</span>))</span><br><span class="line">          <span class="comment"># ......</span></span><br><span class="line">        ]))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(net)</span><br><span class="line"><span class="built_in">print</span>(net[<span class="number">0</span>])</span><br></pre></td></tr></table></figure><p>从上面的过程可以看出，一个完整的流程如下：</p><p>Step1：定义模型</p><p>Step2：传入数据进行前向传播，得到误差</p><p>Step3：反向传播计算梯度，更新梯度，回到St</p>]]></content>
      
      
      
        <tags>
            
            <tag> pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2022/09/15/hello-world/"/>
      <url>/2022/09/15/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="quick-start">Quick Start</h2><h3 id="create-a-new-post">Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="run-server">Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="generate-static-files">Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="deploy-to-remote-sites">Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
